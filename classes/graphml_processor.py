import itertools
import logging
import matplotlib.pyplot as plt
from typing import List, Tuple
import networkx as nx
import numpy as np
import seaborn as sns

from classes.database_client import DatabaseClient


class GraphMLProcessor:
    def __init__(self, db_client: DatabaseClient, processed_tags: List[str], logger: logging.Logger):
        self._db_client = db_client
        self.processed_tags = processed_tags
        self.tag_graphml_mapping = {}
        self.logger = logger

        # generate random colors for each processed tag
        colors = self.generate_distinct_colors(len(self.processed_tags))
        self.color_map = {tag: color for tag, color in zip(self.processed_tags, colors)}

    @staticmethod
    def generate_distinct_colors(n):
        palette = sns.color_palette("hsv", n)  # 'hsv' is a good choice for distinct colors
        return palette.as_hex()

    def set_malware_graphml_paths_for_tags(self):
        """
        Set the malware report paths for the tags.
        """
        self.logger.debug("Setting malware report paths for tags.")

        for tag in self.processed_tags:
            self.logger.debug(f"Finding reports for tag: {tag}")

            tag_like = f"%{tag}%"
            query = f"""
            SELECT
                m.id,
                m.sha256_hash,
                %s AS tag,
                mg.path AS path_to_graphml
            FROM
                visualization.malware_graphml mg
            JOIN 
                malware_bazaar.malware m ON mg.malware_id = m.id
            JOIN
                malware_bazaar.malware_tag mt ON mt.malware_id = mg.malware_id
            JOIN
                malware_bazaar.tag t ON mt.tag_id = t.id AND LOWER(t.tag) LIKE %s
            """
            results = self._db_client.query(query, parameters=(tag, tag_like))

            self.tag_graphml_mapping[tag] = results

    @staticmethod
    def load_and_analyze_graph(file_path: str) -> Tuple[List[int], dict, dict, dict, float]:
        """
        Load and analyze the graph.
        @param file_path: The path to the graphml file.
        @return: The degree distribution, degree centrality, betweenness centrality, and clustering coefficient.
        """
        # Load the graph
        graph = nx.read_graphml(file_path)

        # Calculate metrics
        degree_centrality = nx.degree_centrality(graph)
        betweenness_centrality = nx.betweenness_centrality(graph)
        clustering_coefficient = nx.clustering(graph)
        degree_distribution = [degree for node, degree in graph.degree()]

        # Compute the portion of nodes with in-degree of 1
        in_degrees = graph.in_degree()
        in_degree_one_count = sum(1 for _, degree in in_degrees if degree == 1)
        portion_in_degree_one = in_degree_one_count / graph.number_of_nodes() if graph.number_of_nodes() > 0 else 0

        return degree_distribution, degree_centrality, betweenness_centrality, clustering_coefficient, portion_in_degree_one

    def plot_metric(self, metric_dict: dict, title: str, file_name: str, x_limit: int = None, y_limit: int = None):
        plt.figure(figsize=(10, 6))
        all_averages = []  # To track all averages for setting y limits
        for tag, metrics in metric_dict.items():
            x_values = []
            averages = []

            for i, metric in enumerate(metrics):
                if metric:
                    average = np.mean(metric)
                    averages.append(average)
                    x_values.append(i)

            if averages:
                plt.scatter(x=x_values, y=averages, alpha=0.5, color=self.color_map[tag], label=tag)
                all_averages.extend(averages)  # Add to all averages
            else:
                print(f"No data available for {tag}, skipping...")

        plt.title(title)
        plt.xlabel('Sample Index')
        plt.ylabel('Average Value')
        plt.legend()

        # Adjust x and y axis limits
        if x_values:
            plt.xlim(min(x_values) - 1000, max(x_values) + 1000)  # Add a little extra space on the x-axis
        if all_averages:
            plt.ylim(min(all_averages) - 0.1, max(all_averages) + 0.1)  # Add a little extra space on the y-axis

        if x_limit:
            plt.xlim(0, x_limit)
        if y_limit:
            plt.ylim(0, y_limit)

        plt.savefig(f"{file_name}.png")
        plt.close()

    def process_graphml_files(self):
        degree_distributions = {tag: [] for tag in self.processed_tags}
        degree_centralities = {tag: [] for tag in self.processed_tags}
        betweenness_centralities = {tag: [] for tag in self.processed_tags}
        clustering_coefficients = {tag: [] for tag in self.processed_tags}
        portion_in_degree_one = {tag: [] for tag in self.processed_tags}

        for tag, database_results in self.tag_graphml_mapping.items():
            paths = [result.path_to_graphml for result in database_results]

            for path in paths:
                results = self.load_and_analyze_graph(path)
                degree_distributions[tag].append(results[0])
                degree_centralities[tag].append(list(results[1].values()))
                betweenness_centralities[tag].append(list(results[2].values()))
                clustering_coefficients[tag].append(list(results[3].values()))
                portion_in_degree_one[tag].append(results[4])

        self.plot_metric(degree_distributions, 'Degree Distribution', "degree_distribution")
        self.plot_metric(degree_centralities, 'Degree Centrality', "degree_centrality")
        self.plot_metric(betweenness_centralities, 'Betweenness Centrality', "betweenness_centrality")
        self.plot_metric(clustering_coefficients, 'Clustering Coefficient', "clustering_coefficient")
        self.plot_metric(portion_in_degree_one, 'Portion of Nodes with In-degree of 1', "in_degree_one_portion", y_limit=1)
