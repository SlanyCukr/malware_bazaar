import json
import os
from typing import List

import requests
from bs4 import BeautifulSoup
from tqdm import tqdm

URL = "https://datalake.abuse.ch/malware-bazaar/daily/"
ZIP_FOLDER = "../malware_samples"


def save_malware_exports_status(malware_exports: List[dict]):
    """
    Saves the status of the ZIP files into a JSON file.
    """
    with open("../zip_files.json", "w") as f:
        json.dump(malware_exports, f)


def download_malware_exports(malware_exports: List[dict]):
    """
    Downloads all the ZIP files from the daily exports.
    """
    print("Downloading ZIP files...")

    os.makedirs(ZIP_FOLDER, exist_ok=True)

    # Download each ZIP file
    for malware_export in tqdm(malware_exports):
        if malware_export["status"] == "downloaded":
            continue

        zip_name = os.path.basename(malware_export["url"])
        zip_path = os.path.join(ZIP_FOLDER, zip_name)

        # Get the ZIP file content
        try:
            response = requests.get(malware_export["url"], timeout=30)
            response.raise_for_status()

            # Save the ZIP file
            with open(zip_path, 'wb') as f:
                f.write(response.content)
        except Exception as e:
            print(f"Error downloading {malware_export['url']}.")

            save_malware_exports_status(malware_exports)

            raise e

        # Update the status of the ZIP file
        malware_export["status"] = "downloaded"

    save_malware_exports_status(malware_exports)


def find_malware_exports() -> List[dict]:
    """
    Finds all the ZIP files from the daily exports.
    Also checks if the ZIP file was already downloaded.
    :return: List of ZIP file URLs.
    """
    # Get the HTML content of the URL
    response = requests.get(URL)
    response.raise_for_status()

    # Parse the HTML content using BeautifulSoup
    soup = BeautifulSoup(response.content, "html.parser")

    # Find all the ZIP file URLs
    malware_exports = [os.path.join(URL, a['href']) for a in soup.find_all('a') if
                'href' in a.attrs and a['href'].endswith('.zip')]

    # just return the zip files from BeautifulSoup if the file does not exist
    if not os.path.exists("../zip_files.json"):
        return [{"url": malware_export, "status": "not_downloaded"} for malware_export in malware_exports]

    # load the previous zip files from json
    with open("../zip_files.json", "r") as f:
        previous_malware_exports = json.load(f)

    # create a list of dictionaries with name and status for each zip file
    current_malware_exports = [{"url": malware_export, "status": "not_downloaded"} for malware_export in malware_exports]

    # join the two lists of dictionaries, take status from previous_zip_files if the zip file was already downloaded
    for current_malware_export in current_malware_exports:
        for previous_malware_export in previous_malware_exports:
            if current_malware_export["url"] == previous_malware_export["url"]:
                current_malware_export["status"] = previous_malware_export["status"]

    return current_malware_exports


if __name__ == "__main__":
    malware_exports: List[dict] = find_malware_exports()

    download_malware_exports(malware_exports)
